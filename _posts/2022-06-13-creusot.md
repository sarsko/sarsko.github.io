---
title: "Some tips on doing program proofs in Creusot"
date: 2022-06-13T15:34:30-04:00
categories:
  - Creusot
tags:
  - Creusot
---

-- This is a Work In Progress! Going to add more stuff soonâ„¢ --

The following is a sort of guide to Creusot, based on my experience in using it
to verify SAT solvers as a part of my masters. It is based on the observation
that it would be very nice if there were more resources for learning deductive
verification. There may be a case for making a general guide for learning
deductive verification: that is for another day and most likely someone else &mdash;
this is a guide for learning Creusot. This is my attempt at distilling some of
my learnings into something which I hope is helpful for those attempting to learn
either deductive verification, or Creusot in particular.

I don't claim to be some higher being of divine verification prowess, and this
work is neither complete, nor (I would assume) perfect. If you have additions
you would like to suggest, or corrections to what is in here, please file a pull
request, or message/mail me. I believe my relative "youth" in proving programs
is to some degree a strength &mdash; hopefully I remember more of the pain-points (that
being said, if you are some higher being of divine verification prowess, please
a guide, I would love to read it).

It is in a very limited way complementary (speaking of compliments, you are looking
particularly nice today) to my thesis &mdash; some examples will be from it
it, and most of my experience is from writing and verifying SAT solvers, but you
don't have to become an expert at verifying SAT solvers if you don't want to.

## Before starting

### Verification can be painful

I have found formal verification to be an entirely new kind of pain, and I believe this is part of the draw of formal verification.

Most things in programming are some hybrid between a search procedure and a logic puzzle. For instance:
- When doing "regular programming", it is usually possible to find the solution to your problem on StackOverflow or in the documentation for
whatever programming language or framework or API or whatever which you are trying to get to work. 
- In the case you are debugging, you are searching for some bug, and using logic to prune the search space.
- When doing some low level stuff or working with the internals of some external tool, the process may be harder, but the answer is still in the code (or in 
[Ralf Brown's interrupt list](https://www.ctyme.com/rbrown.htm)).

When we are doing a program proof, we are still doing a hybrid between a search procedue and a logic puzzle. There are howevever two importan differences.
The first is that we are searching for two things at once: a program which does what we want, and a proof which is able to prove that program. The second is
that the answer can only be found somewhere within our head &mdash; there is no answer on StackOverflow, or some library function which we can call. This is
also why I believe the pain is part of the draw of formal verification. When we found a solution, we didn't just look it up, we had to conjure it in our mind.
If we did not immediately find the proof, this means that we most likely ended up learning something, and that something was, I would argue, much more fundamental
than knowing what interrupt to invoke or what API endpoint to connect to.

### Verification can be hard

### Verification can be tedious


### Verification can be fun

I have already touched on this in the section about pain, but the satisfaction which can be gotten from doing proofs is really quite addictive. People aren't attending
VerifyThis or adding proof to the Isabelle Archive of Proofs just because they want to acquire the begrudging admiration of a few nerds &mdash; they do, as wicked as it may
seem, actually think doing proofs is _fun_.
### Verification can be useful

Okay, this is obviously a joke. Haha.

## Introduction

### Intro to syntax

### Using the syntax

### A bigger example

## Some tips

### Be nice to the SMT solvers

The SMT solvers are your judge. They are in some ways really smart, and in some ways really dumb. If you find yourself asking: "Do I really have to spell it
out for you", then yes, yes you should. Sometimes this means that you have to break abstraction barriers, which is really _not nice_, but if that is what it
takes to make the SMT solvers happy, then we'll have to do it for now, and fix it later. The same applies when writing your invariants and your specs. If we
can reword the invariant to not take a quantifier, for instance by doing a change in the proof, then we should probably do that. 

### Know your program

Sometimes, especially when starting out, our ability to do program proofs is limited by our lack of expertise in using Creusot. As we get better, this becomes
much more rare, and our ability to do program proofs becomes more and more limited by our lack of expertise in the program which we are going to prove. 

### Good software practice is great Creusot practice

Creusot charges a much higher interest rate on technical debt and poor choices
than what one may be used to. This means that either you will have to apply
all the stuff you have heard about software best practices, or you will endure a
world of pain. The reason this happens is two-fold: the first is that a program
which was 200 lines, may become 1000 lines when adding proof code. This means that you have the
functionality of 200 lines, but the cognitive burden of 1000. The second reason
is that poorly structured code gives poorly structured proofs, which are even
more impossible to make into well structured proofs than it is to make your code
into well structured code.

Keep in mind: \
Creusot increases the magnitude of your "quality of code"-vector.


#### Encapsulation

I didn't write Java for very long, but the little I had to do, made me develop a
disdain for excessive encapsulation. I later got into C, and, as all good C
programmers do, I encapsulated as little as possible. That stuff
may cost CPU cycles, and CPU cycles are what it is all about.

When writing programs to be verified with Creusot: \
__do use encapsulation__.

The reason for this is tied to a few things:
- As mentioned before, your project will become many times the size of its functionality.
    A change in how something is represented one place in your code may lead to
    the changing of 10 proofs. Proofs are at least as fragile as regular code,
    you don't want to open for the possibility of you messing them up.
- SMT-solvers like patterns. Your underlying datastructures often exhibit less
    patterns than their abstraction. Remember the mantra: __Be nice to
    your SMT-solvers__.
  - An addendum to the point above: Sometimes one encapsulates things, and the
    proof stops checking out. This is usually due to the SMT solvers not
    "peeking" into your encapsulated data structure, and thus not seeing the
    structure which made the proof pass before. A tempting solution is to undo
    the encapsulation, as, after all, things were checking out before. I
    recommend avoiding this temptation, as this is usually indicative of your
    data structures at some later point becoming too convoluted for the SMT
    solvers to reason about. In other words: do this only if you are short on
    time and 100% sure your data structure will not change, nor be used in any
    other proof. A better solution is to annotate your new encapsulations with
    their bodies (yes, this is not ideal, but it is better than no encapsulation),
    or to redo parts of the proof. I am fairly certain you will see that the
    proof checks out faster than before, and that your new abstractions will
    make you and your SMT solvers happy in the time to come.
- When writing a proof, you are not just figuring out the best representation
    for whatever metrics you like your code to have (speed, usability,
    maintainability of the code etc.), but also the best representation to make
    the proofs easy to do for you, as well as easy to do for the SMT solvers (__Be
    nice to your SMT-solvers__). If you want to stay nimble, properly encapsulate
    your stuff.

#### Low coupling

This is one of those things which everybody would like to have for all their
code ("Ah I really want my code to be modular and maintainable"), but which is
somewhat hard to achieve in practice (or rather, it is easy to not achieve it.
The hard thing is refraining from the poor choices which lead to high coupling,
and also to think before we write our code.). I am not sure if there exists a
concept similar to Kolmogorov complexity, but for how connected information
is (I'm sure there does), but we want to stay as close to that as possible. The
good thing is that the Rust type system nudges us in the right direction (we
can't do all the crazy stuff we can do in C/C++), and we only have extend that a
little bit further.

The reason for this is simple:
- One large proof is infinitesimally harder than many small. Okay, maybe not
    entirely, but the "hardness" of proofs does not scale linearily. Many
    smaller proofs is much better than one large. This point ties together with
    the "Properties" subchapter of the next chapter.

### Optimize for proof ergonomics, not code ergonomics

Very often, code which is easy to prove is code which is easy to reason about,
and vice versa. Sometimes this is not the case, and you **should always favor
writing code which is easy to prove over code which is easy to write**. The
situation I have faced, and I believe many will face (as it is useful fairly
often), is to have vectors which are padded. This is useful when a somewhat
frequent operation on your vector is to compare an element with its neighbor.

The problem with doing this is that you have traded away ergonomics in the proof
for ergonomics in the code. Suddenly all the functions which operate on your
vector has to require (and ensure) that the vector is padded. It would have been
much better to write a little bit more code in the function where having padding
would be nice, than to introduce this pervasive contract.

(could mention something here about how one could make it into the type or make
transformer functions and that making fragile functions is bad anyways?)

## Filthy rewrites

### You have to rewrite

### The hierarchy of "provenness"

There is what one may call a "hierarchy for "provenness"" which it is
surprisingly easy to get wrong. The reason one so often gets this wrong, is the
mistaken belief that the highest rung is to statically guarantee runtime safety
and full correctness, with termination. Of course this is nice and all, but the
highest rungs are reserved for programs which are *useful*. Said bluntly: I
would rather have a program which typechecks and runs fast, than one which has
all the correctness and safety-guarantees, but runs slow.

How much are you willing to pay to guarantee termination? Are you willing to
incur one runtime check to gain 2x speedup?

As a practical example, consider CreuSAT, my SAT solver. For a long time, I did
not want to do runtime checks, and I did definitely not want to make it
explicitly ternary, meaning that I did not want it to have have the possibility of returning some
error-value. Then, at some point, I realized that I could save many hundreds
of lines of proof by doing one runtime check in the case the formula was SAT. Is the performance going to be affected
by one linear pass? No. Did it allow me to move on and prove
more important things which did result in a noticeable speedup? Yes.

Later on I realized that I could save myself a significant amount of time by
allowing my solver to return `Err` and not proving correctness for certain
parts. I knew the `Err` state would never be reached, but I also knew that
proving so would take a significant amount of time. What do users of SAT solvers
care about? Speed. All solvers have an either implicit or explicit error state,
which is that the solver times out. Most people would take the theoretical
possibility of recieving an explicit error, if it gave them a solver which is X
times faster.

In short:
- Think about what the important metrics for your application is, and the various tradeoffs you can do in terms of what you prove to achieve those metrics.

The hierarchy, more or less. Sure, there may be cases where proving termination is more important than even correctness or safety:
1. Useful and statically proven safety, correctness, completeness, and termination.
2. Useful and statically proven safety, correctness, and either completeness or termination.
2. Useful and statically proven safety and correctness.
3. Useful with runtime checks and statically proven correctness and termination.
4. Useful with runtime checks and statically proven correctness.
5. Useful with type safety.
6. Useless. Who cares what you have proven, your program is useless.

## More advanced topics

### Lemma functions

#### Pro tip: Lemmas as tests

This is a concept which I realized a bit late, and which I utilized less than I should, but it is a
very powerful concept, which greatly improves the process of refactoring code.

The idea is really simple: we can use lemmas to test abstract properties which should hold, such as that pushing and poping to a vector
returns the same vector, or that taking the absolute value of a number multiple times is the same as doing it once. A key thing here is
that lemmas should, to the degree it is possible, work on the abstractions of the data structures. This means that if we ever refactor our
data structures, for instance by swapping a vector representation to a set representation, either our lemmas will break, in which case we
can make the choice to either fix them (or the implementation) or remove them, or they will continue passing, in which case our refactoring
was sound. 

Lemma tests should be designed to be faster to prove than the rest of the proof (it is fine to have lemmas which take
more time to prove, and which are run less often), and are used to quickly check whether a given refactoring is correct, or
if some change to Creusot breaks some core functionality. They should (for the most part) be entirely independent of the functionality
of the program, and be used to test invariants of the data structures and their associated functions.

Lemma tests should be in a separate file and should always be private, to ensure that they are not accidentally used in the proof. Ideally one
should use `#[cfg_attr(feature = "no_tests", trusted)]` or `#[cfg(feature = "test")]` to not have to run them every single time the proof is run.

__Lemma driven development?__

The natural follow-up to using lemmas as tests is whether "lemma driven development" is a good idea or not. I do not know &mdash; I have not tried.
Though I do believe that doing a sort of "contract driven development" sometimes is a good idea, I have never thought "Oh, I should do "lemma driven development"".
A variant of this, which I have done, is to specify a lemma which does not prove, but which I believe should prove, and then use that, and then later
fill inn the rest of the lemma, but this is not exactly the same. 

### "Reaching up" to Why3

### Thinking in terms of properties



## Some more advanced tips

### (Repeat the tip from earlier)

### Split borrows

## Some practical tips and tricks for using Creusot

### Pass-through and `#[cfg(feature = "contracts")]``

All calls to `cargo creusot` in CreuSAT which are done through the `cargo make` commands are done with the `--features=contracts`,
which includes the parts which are only used for the proofs, such as defining models for various data structures, or including
the files which have the `logic_` prefix. This enables pass through when running regular `cargo` commands, as things like defining
models will cause the compiler to error otherwise. Before pass-through mode was enabled, I would maintain a mirrored version of the
solvers, which was *super not nice*, and something which I would *not recommend*.

### Using cargo make (or aliasing mlcfg and why3-ide)

### Using `#[cfg_attr(feature = "trust_assignments", trusted)] and `#[trusted]`

If you look at the source code of CreuSAT, you will see that most of the functions are annotated by something similar to
`#[cfg_attr(feature = "trust_assignments", trusted)]`. This allows us to run `cargo creusot` with `--features=trust_assignments`
and have all the functions which have this annotation be trusted, and thus removed from the generated MLCFG. This means that
we can load only the functions which we are currently trying to prove, thereby reducing the iteration time. Be wary of adding
permanent `#[trusted]` annotations to functions, as you risk forgetting about them.

### The "strict resource count" discipline

All right, so this is a bit of a TODO as I need to dirty my hands a lot more.

This is an idea / discipline which I got introduced to from the people doing proofs in Dafny, and which is described to some degree in some of the more recent papers about program proofs in Dafny. 

The idea is very simple:
1. Keep track of how much "resources" (some abstract Resource Count (RC)) each verification condition requires to pass.
2. Maintain the following two invariants:
     1. All VCs should pass within some resouce bound.
     2. Regressions (ie. RC<sub>new</sub> * 1.5 >= RC<sub>prev</sub>) == proof failure.

The resource bound should be set such that it is significantly below the timeout limit for the SMT solvers.

The discipline is nice, because it ensures that **proofs always pass**.

The first invariant solves the following somewhat common issue:
1. Some VC takes a lot of time. It should be split up, but you can't be bothered, because, hey, it is passing after all.

The second invariant solves the following (unfortunately common) issues:
1. You do some refactoring. Seemingly unrelated parts of the proof now take more time.
2. You update your SMT solvers. Some VCs now take more time.

In short: such a discipline is the only way I know of which ensures that you don't get encumbered by *proof debt*, which at some point makes your proof basically impossible to work with.